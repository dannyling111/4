# Import necessary modules and libraries
import time
import pandas as pd
from fastapi_poe.types import ProtocolMessage
from fastapi_poe.client import get_bot_response
import json
import asyncio
from datetime import datetime
import requests
from PIL import Image
import os
import re
import streamlit as st
from requests.exceptions import MissingSchema
import google.generativeai as genai
from pytrends.request import TrendReq
from collections import Counter
from wordcloud import WordCloud
import random
from io import BytesIO
import matplotlib.pyplot as plt
import openpyxl
import urllib.parse

# Configure Matplotlib to use 'Agg' backend for Streamlit compatibility
plt.switch_backend('Agg')

# Load API keys and configure models
api_key = st.secrets["api_keys"]["my_api_key"]
gemini_api_key = st.secrets["api_keys"]["gemini_api_key"]
genai.configure(api_key=gemini_api_key)

gemini_bots = ["gemini-1.5-flash", "gemini-1.5-pro", "gemini-1.0-pro"]
image_bots = ["DALL-E-3", "FLUX-pro"]
text_bots = gemini_bots + ["GPT-3.5-Turbo", "GPT-4", "Claude-3-Opus"]

painting_styles = ["", "æ²¹ç”»", "æ°´å½©ç”»", "æ°´å¢¨ç”»", "ç´ æ", "ä¸™çƒ¯ç”»", "å†™å®ä¸»ä¹‰", "å°è±¡æ´¾", "æ¢µé«˜é£æ ¼", "è¾¾èŠ¬å¥‡é£æ ¼", "è«å¥ˆé£æ ¼", 
                   "æ¯•åŠ ç´¢é£æ ¼", "å¾·åŠ é£æ ¼", "é›·è¯ºé˜¿é£æ ¼", "ç±³å¼€æœ—åŸºç½—é£æ ¼", "è¾¾åˆ©é£æ ¼", "é«˜æ›´é£æ ¼", "åº·å®šæ–¯åŸºé£æ ¼", 
                   "å¡å°šé£æ ¼", "çˆ±å¾·åÂ·é©¬å¥ˆé£æ ¼", "é½ç™½çŸ³é£æ ¼", "å¼ å¤§åƒé£æ ¼", "å¾æ‚²é¸¿é£æ ¼", "å´å† ä¸­é£æ ¼"]

# Load the Excel file (replace the file path with your .xlsx file)
xlsx_path = "aisetting.xlsx"  # Ensure the correct path to your Excel file
aisettings_df = pd.read_excel(xlsx_path)

# Extract language and thinking options
language_options = aisettings_df['a1'].dropna().tolist()
thinking_options = aisettings_df['a2'].dropna().tolist()

# General-purpose functions


def fetch_image_response(image_prompt, model):
    async def fetch():
        message = ProtocolMessage(role="user", content=image_prompt)
        reply = ""
        async for partial in get_bot_response(messages=[message], bot_name=model, api_key=api_key):
            response = json.loads(partial.raw_response["text"])
            reply += response["text"]
        return reply

    return asyncio.run(fetch())

def get_google_trends():
    pytrends = TrendReq(hl='en-US', tz=360)
    countries = ['united_states', 'japan', 'hong_kong', 'united_kingdom', 'taiwan', 'india', 'singapore', 'australia']
    trends_list = []
    for country in countries:
        trends = pytrends.trending_searches(pn=country)
        trends_list.extend(trends.values.tolist())
    return [item[0].strip() for item in trends_list]

def generate_wordcloud(keywords):
    font_path = 'NotoSansCJK-Regular.ttc'  # Update to your font path
    wordcloud = WordCloud(
        font_path=font_path,
        width=1600, 
        height=1600, 
        background_color="white",
        color_func=muted_color_func
    ).generate_from_frequencies(Counter(keywords))

    plt.figure(figsize=(10, 10), dpi=100)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")

    buf = BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)
    return buf, wordcloud.words_

def muted_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    h = random.randint(180, 360)
    s = random.randint(50, 80)
    l = random.randint(40, 50)
    return f"hsl({h}, {s}%, {l}%)"


# Page Block 1: Keyword Extraction
def keyword_extraction_page():
    st.header("å…³é”®è¯æå–å’Œæœç´¢é“¾æ¥ç”Ÿæˆ")

    # Check if we need to load a keyword as input (from a previous keyword click)
    if 'input_text_prompt' not in st.session_state:
        st.session_state.input_text_prompt = ''  # Initialize if not present

    # Set the input text from session state (this could come from a keyword button click)
    input_text_prompt = st.text_input("è¯·è¾“å…¥æ–‡æœ¬ç”Ÿæˆæç¤ºè¯", value=st.session_state.input_text_prompt)
    
    selected_language = st.selectbox("é€‰æ‹©è¯­è¨€", language_options)
    selected_text_model = st.selectbox("é€‰æ‹©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹", text_bots)
    fixed_prompt_options = aisettings_df['a5'].dropna().tolist()
    selected_fixed_prompt = st.selectbox("é€‰æ‹©å…³é”®è¯ç”Ÿæˆæ¨¡æ¿", fixed_prompt_options)

    # Initialize session state to hold multiple rounds of keyword generation
    if 'keywords_rounds' not in st.session_state:
        st.session_state.keywords_rounds = []  # Empty list to hold each round of keywords

    # Button to generate new round of keywords
    if st.button("ç”Ÿæˆå…³é”®è¯å’Œé“¾æ¥"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå…³é”®è¯å’Œé“¾æ¥..."):
            # Generate new keywords
            new_keywords = generate_keywords_and_links(
                input_text_prompt, selected_language, selected_text_model, selected_fixed_prompt
            )

            # Append the new keywords to session state (multiple rounds of keywords)
            st.session_state.keywords_rounds.append(new_keywords)

    # Button to clear results
    if st.button("æ¸…é™¤ç»“æœ"):
        st.session_state.keywords_rounds = []  # Clear all previous rounds of keywords
        st.session_state.input_text_prompt = ''  # Clear the input text
        st.success("æ‰€æœ‰ç»“æœå·²æ¸…é™¤ï¼")

    # Display all rounds of keywords
    if st.session_state.keywords_rounds:
        for round_idx, keywords in enumerate(st.session_state.keywords_rounds, 1):
            st.subheader(f"ç¬¬ {round_idx} è½®ç”Ÿæˆçš„å…³é”®è¯")
            display_keywords_and_links(
                keywords, input_text_prompt, selected_language, selected_text_model, selected_fixed_prompt, round_idx
            )


# ä¿®æ”¹åçš„generate_keywords_and_linkså‡½æ•°
def generate_keywords_and_links(input_text, language, model, fixed_prompt_append):
    # æ„é€ åŒ…å«è¯­è¨€é€‰é¡¹çš„æœ€ç»ˆæç¤ºè¯
    final_prompt = f"{input_text}\n{fixed_prompt_append}\nLanguage: {language}" if language else f"{input_text}\n{fixed_prompt_append}"

    async def fetch_text_response():
        message = ProtocolMessage(role="user", content=final_prompt)
        reply = ""
        async for partial in get_bot_response(messages=[message], bot_name=model, api_key=api_key):
            response = json.loads(partial.raw_response["text"])
            reply += response["text"]
        return reply

    text_response = asyncio.run(fetch_text_response())
    if text_response:
        try:
            keywords = [line.strip()[2:] for line in text_response.splitlines() if line.startswith("-")]
            return keywords
        except Exception as e:
            st.error(f"Error processing keywords: {str(e)}")
            return []

    text_response = asyncio.run(fetch_text_response())
    if text_response:
        try:
            keywords = [line.strip()[2:] for line in text_response.splitlines() if line.startswith("-")]
            return keywords
        except Exception as e:
            st.error(f"Error processing keywords: {str(e)}")
            return []

def display_keywords_and_links(keywords, input_text, selected_language, selected_text_model, fixed_prompt_append, round_idx):
    for idx, keyword in enumerate(keywords):
        col1, col2, col3 = st.columns([3, 1, 1])  # Layout with 3 columns

        with col1:
            st.markdown(f"{keyword}")
        
        with col2:
            google_search = f"https://www.google.com/search?q={keyword}"
            youtube_search = f"https://www.youtube.com/results?search_query={keyword}"
            bilibili_search = f"https://search.bilibili.com/all?keyword={keyword}"
            st.markdown(f"[Google]({google_search}) | [YouTube]({youtube_search}) | [Bilibili]({bilibili_search})")
        
        with col3:
            # Ensure unique key by combining round index, loop index, and the keyword
            button_key = f"regen_{round_idx}_{idx}_{keyword}"
            if st.button(f"ğŸ”„ é‡æ–°ç”Ÿæˆ {keyword}", key=button_key):
                # Use the clicked keyword as the new input and regenerate
                st.session_state.input_text_prompt = keyword

                # Generate new keywords from the clicked keyword (without rerun)
                new_keywords = generate_keywords_and_links(
                    input_text=keyword,  # Use current keyword as new input
                    language=selected_language,  # Reuse existing language setting
                    model=selected_text_model,  # Reuse existing model setting
                    fixed_prompt_append=fixed_prompt_append  # Reuse existing prompt append
                )
                if new_keywords:
                    st.session_state.keywords_rounds.append(new_keywords)  # Add to session state for history
def image_generation_page():
    st.header("å›¾åƒç”Ÿæˆ")
    input_image_prompt = st.text_input("è¯·è¾“å…¥å›¾åƒç”Ÿæˆæç¤ºè¯")
    selected_style = st.selectbox("é€‰æ‹©ç»˜ç”»é£æ ¼", painting_styles, index=0)
    selected_image_model = st.selectbox("é€‰æ‹©å›¾åƒç”Ÿæˆæ¨¡å‹", image_bots)

    if st.button("ç”Ÿæˆå›¾åƒ"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆå›¾åƒ..."):
            image_prompt = f"{input_image_prompt}ï¼Œé£æ ¼ä¸º{selected_style}" if selected_style else input_image_prompt
            image_response = fetch_image_response(image_prompt, selected_image_model)

            image_url = re.search(r'\((.*?)\)', image_response).group(1)
            try:
                image_data = requests.get(image_url).content
                st.image(image_data, caption="ç”Ÿæˆçš„å›¾åƒ", use_column_width=True)
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½å›¾åƒ: {str(e)}")

# Page Block 3: Text Generation
def text_generation_page():
    st.header("æ–‡æœ¬ç”Ÿæˆ")
    input_text_prompt = st.text_input("è¯·è¾“å…¥æ–‡æœ¬ç”Ÿæˆæç¤ºè¯")
    selected_language = st.selectbox("é€‰æ‹©è¯­è¨€", [''] + language_options)
    selected_thinking = st.selectbox("é€‰æ‹©æ€ç»´æ–¹å¼", [''] + thinking_options)
    selected_a3_items = st.multiselect("é€‰æ‹©é™„åŠ é¡¹ (a3 åˆ—)", aisettings_df['a3'].dropna().tolist())
    selected_text_model = st.selectbox("é€‰æ‹©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹", text_bots)

    message_content = input_text_prompt
    if selected_language:
        message_content += f"\nLanguage: {selected_language}"
    if selected_thinking:
        message_content += f"\nThinking: {selected_thinking}"
    if selected_a3_items:
        message_content += f"\nAdditional: {', '.join(selected_a3_items)}"

    st.subheader("AI ç”Ÿæˆçš„æœ€ç»ˆæç¤ºè¯ï¼š")
    st.write(f"Prompt Input:\n{message_content}")

    if st.button("ç”Ÿæˆæ–‡æœ¬"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆæ–‡æœ¬..."):
            text_response = fetch_text_response(message_content, selected_text_model)
            if text_response:
                st.success("ç”Ÿæˆçš„æ–‡æœ¬ï¼š")
                st.write(text_response)

# Page Block 4: Word Cloud Generation
def wordcloud_generation_page():
    st.header("ç”Ÿæˆ Google Trends è¯äº‘")
    if st.button("ç”Ÿæˆè¯äº‘"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆè¯äº‘..."):
            trend_keywords = get_google_trends()
            wordcloud_image, keyword_frequencies = generate_wordcloud(trend_keywords)

            st.image(wordcloud_image, caption="Google Trends è¯äº‘", use_column_width=True)

            sorted_keywords = sorted(keyword_frequencies.items(), key=lambda x: x[1], reverse=True)[:20]
            st.subheader("å…³é”®è¯æœç´¢é“¾æ¥")
            for keyword, _ in sorted_keywords:
                google_search_link = f"https://www.google.com/search?q={keyword}"
                google_news_link = f"https://news.google.com/search?q={keyword}"
                youtube_link = f"https://www.youtube.com/results?search_query={keyword}"
                st.markdown(f"- {keyword}: [Google Search]({google_search_link}) | [Google News]({google_news_link}) | [YouTube]({youtube_link})")



# åœ¨ display_analysis_keywords å‡½æ•°ä¸­æ·»åŠ æ¨¡æ¿é€‰æ‹©ä¸‹æ‹‰æ¡†
def display_analysis_keywords(keywords, selected_language, selected_text_model, fixed_prompt_options_a6, round_idx, generate_links, fixed_prompt_used):
    if not keywords:
        st.error("No keywords provided.")
        return

    for idx, keyword in enumerate(keywords):
        col1, col2, col3 = st.columns([3, 2, 3])  # è°ƒæ•´åˆ—å®½

        with col1:
            st.markdown(f"**{keyword}**")
            if generate_links:
                encoded_keyword = urllib.parse.quote(keyword)
                google_search = f"https://www.google.com/search?q={encoded_keyword}"
                youtube_search = f"https://www.youtube.com/results?search_query={encoded_keyword}"
                bilibili_search = f"https://search.bilibili.com/all?keyword={encoded_keyword}"
                st.markdown(f"[Google]({google_search}) | [YouTube]({youtube_search}) | [Bilibili]({bilibili_search})")

        with col2:
            action_key = f"action_select_{round_idx}_{idx}"
            action_options = ["è¯·é€‰æ‹©æ“ä½œ", "ğŸ”„ é‡æ–°ç”Ÿæˆå…³é”®è¯", "ğŸ“ ç”Ÿæˆåˆ†ææ–‡ç« "]
            selected_action = st.selectbox(
                "é€‰æ‹©æ“ä½œ",
                options=action_options,
                key=action_key
            )
            action_processed_key = f"action_processed_{round_idx}_{idx}"

            if selected_action != "è¯·é€‰æ‹©æ“ä½œ" and not st.session_state.get(action_processed_key, False):
                if selected_action == "ğŸ”„ é‡æ–°ç”Ÿæˆå…³é”®è¯":
                    with st.spinner(f"æ­£åœ¨ä½¿ç”¨å…³é”®è¯ {keyword} é‡æ–°ç”Ÿæˆ..."):
                        new_keywords = generate_keywords_and_links(
                            input_text=keyword,
                            language=selected_language,
                            model=selected_text_model,
                            fixed_prompt_append=fixed_prompt_used  # ä½¿ç”¨å½“å‰è½®æ¬¡çš„æ¨¡æ¿
                        )
                        if new_keywords:
                            st.session_state.analysis_rounds.append({
                                'type': 'keywords',
                                'content': new_keywords,
                                'generate_links': generate_links,
                                'fixed_prompt': fixed_prompt_used  # ç»§ç»­ä¼ é€’ä½¿ç”¨çš„æ¨¡æ¿
                            })
                            st.session_state[action_processed_key] = True
                            st.experimental_rerun()
                elif selected_action == "ğŸ“ ç”Ÿæˆåˆ†ææ–‡ç« ":
                    with st.spinner(f"æ­£åœ¨ç”Ÿæˆå…³äº {keyword} çš„åˆ†ææ–‡ç« ..."):
                        analysis_prompt = f"å†™ä¸€ç¯‡å…³äº{keyword}çš„åˆ†ææ–‡ç« ã€‚è¯­è¨€: {selected_language}"
                        analysis_article = fetch_text_response(analysis_prompt, selected_text_model)
                        if analysis_article:
                            st.session_state.analysis_rounds.append({
                                'type': 'article',
                                'content': analysis_article
                            })
                            st.session_state[action_processed_key] = True
                            st.experimental_rerun()

        with col3:
            # æ·»åŠ æ¨¡æ¿é€‰æ‹©ä¸‹æ‹‰æ¡†
            template_key = f"template_select_{round_idx}_{idx}"
            previous_template_key = f"previous_template_{round_idx}_{idx}"

            selected_template = st.selectbox(
                "é€‰æ‹©å…³é”®è¯ç”Ÿæˆæ¨¡æ¿",
                options=['è¯·é€‰æ‹©æ¨¡æ¿'] + fixed_prompt_options_a6,
                key=template_key
            )

            previous_template = st.session_state.get(previous_template_key, 'è¯·é€‰æ‹©æ¨¡æ¿')

            if selected_template != 'è¯·é€‰æ‹©æ¨¡æ¿' and selected_template != previous_template:
                with st.spinner(f"æ­£åœ¨ä½¿ç”¨å…³é”®è¯ {keyword} å’Œæ¨¡æ¿ {selected_template} é‡æ–°ç”Ÿæˆ..."):
                    new_keywords = generate_keywords_and_links(
                        input_text=keyword,
                        language=selected_language,
                        model=selected_text_model,
                        fixed_prompt_append=selected_template
                    )
                    if new_keywords:
                        st.session_state.analysis_rounds.append({
                            'type': 'keywords',
                            'content': new_keywords,
                            'generate_links': generate_links,
                            'fixed_prompt': selected_template  # å­˜å‚¨æ–°é€‰æ‹©çš„æ¨¡æ¿
                        })
                        st.session_state[previous_template_key] = selected_template
                        st.experimental_rerun()

def analysis_generation_page():
    st.header("ä¸»é¢˜åˆ†æç”Ÿæˆ")

    if 'input_text_prompt_analysis' not in st.session_state:
        st.session_state.input_text_prompt_analysis = ''
    if 'analysis_rounds' not in st.session_state:
        st.session_state.analysis_rounds = []
    if 'trigger_rerun' not in st.session_state:
        st.session_state.trigger_rerun = False

    input_text_prompt_analysis = st.text_input(
        "è¯·è¾“å…¥æ–‡æœ¬ç”Ÿæˆæç¤ºè¯", value=st.session_state.input_text_prompt_analysis)

    selected_language = st.selectbox("é€‰æ‹©è¯­è¨€", language_options)
    selected_text_model = st.selectbox("é€‰æ‹©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹", text_bots)

    fixed_prompt_options_a6 = aisettings_df['a6'].dropna().tolist()
    selected_fixed_prompt_a6 = st.selectbox("é€‰æ‹©å…³é”®è¯ç”Ÿæˆæ¨¡æ¿", fixed_prompt_options_a6)

    generate_links = st.checkbox("æ˜¯å¦ç”Ÿæˆå…³é”®è¯ç›¸å…³çš„æœç´¢é“¾æ¥", value=True)

    if st.button("ç”Ÿæˆå…³é”®è¯"):
        if input_text_prompt_analysis.strip():
            with st.spinner("æ­£åœ¨ç”Ÿæˆå…³é”®è¯..."):
                new_analysis_keywords = generate_keywords_and_links(
                    input_text_prompt_analysis, selected_language, selected_text_model, selected_fixed_prompt_a6)

                if new_analysis_keywords:
                    st.session_state.analysis_rounds.append({
                        'type': 'keywords',
                        'content': new_analysis_keywords,
                        'generate_links': generate_links,
                        'fixed_prompt': selected_fixed_prompt_a6  # å­˜å‚¨ä½¿ç”¨çš„æ¨¡æ¿
                    })
                    st.session_state.trigger_rerun = True
        else:
            st.warning("è¯·è¾“å…¥æ–‡æœ¬ç”Ÿæˆæç¤ºè¯ï¼")

    if st.button("æ¸…é™¤ç»“æœ"):
        st.session_state.analysis_rounds = []
        st.session_state.input_text_prompt_analysis = ''
        st.session_state.trigger_rerun = True
        st.success("æ‰€æœ‰ç»“æœå·²æ¸…é™¤ï¼")

    for round_idx, round_data in enumerate(st.session_state.analysis_rounds):
        if round_data['type'] == 'keywords':
            st.subheader(f"ç¬¬ {round_idx + 1} è½®ç”Ÿæˆçš„ä¸»é¢˜å…³é”®è¯")
            display_analysis_keywords(
                round_data['content'],
                selected_language,
                selected_text_model,
                fixed_prompt_options_a6,  # ä¼ é€’æ¨¡æ¿é€‰é¡¹åˆ—è¡¨
                round_idx,
                round_data['generate_links'],
                round_data['fixed_prompt']  # ä¼ é€’å½“å‰è½®æ¬¡ä½¿ç”¨çš„æ¨¡æ¿
            )
        elif round_data['type'] == 'article':
            st.subheader(f"åˆ†ææ–‡ç« ï¼šç¬¬ {round_idx + 1} è½®")
            st.write(round_data['content'])

    # Check if a rerun is needed and reset the trigger
    if st.session_state.trigger_rerun:
        st.session_state.trigger_rerun = False
        st.experimental_rerun()


def rerun_with_keyword(keyword, selected_language, selected_text_model, fixed_prompt_append):
    with st.spinner(f"æ­£åœ¨ä½¿ç”¨å…³é”®è¯ {keyword} é‡æ–°ç”Ÿæˆ..."):
        new_keywords = generate_keywords_and_links(keyword, selected_language, selected_text_model, fixed_prompt_append)
        if new_keywords:
            st.session_state.analysis_rounds.append({
                'type': 'keywords',
                'content': new_keywords,
                'generate_links': True  # Assuming we want links for rerun keywords
            })
          
# Fetch the analysis article using the API call
def fetch_text_response(message_content, model):
    async def fetch():
        message = ProtocolMessage(role="user", content=message_content)
        reply = ""
        async for partial in get_bot_response(messages=[message], bot_name=model, api_key=api_key):
            response = json.loads(partial.raw_response["text"])
            reply += response["text"]
        return reply

    return asyncio.run(fetch())


def japanese_learning_page():
    st.header("å­¦ä¹ æ—¥è¯­")
    input_text_prompt = st.text_input("è¯·è¾“å…¥ä¸å­¦ä¹ æ—¥è¯­ç›¸å…³çš„æç¤ºè¯")
    
    # Extract options for a4 column (é™„åŠ é¡¹) and change to single select (å•é€‰)
    selected_a4_item = st.selectbox("é€‰æ‹©é™„åŠ é¡¹ (a4 åˆ—)", [''] + aisettings_df['a4'].dropna().tolist())  # æ”¹ä¸ºå•é€‰

    # Select language (å•é€‰)
    selected_language = st.selectbox("é€‰æ‹©è¯­è¨€", [''] + language_options)  # å•é€‰è¯­è¨€
    
    # Select text generation model
    selected_text_model = st.selectbox("é€‰æ‹©æ–‡æœ¬ç”Ÿæˆæ¨¡å‹", text_bots)

    # Construct final prompt
    message_content = input_text_prompt
    if selected_language:
        message_content += f"\nLanguage: {selected_language}"
    if selected_a4_item:
        message_content += f"\né™„åŠ é¡¹: {selected_a4_item}"  # å•é€‰é™„åŠ é¡¹
    
    st.subheader("AI ç”Ÿæˆçš„æœ€ç»ˆæç¤ºè¯ï¼š")
    st.write(f"**Prompt Input:**\n{message_content}")

    if st.button("ç”Ÿæˆæ–‡æœ¬"):
        with st.spinner("æ­£åœ¨ç”Ÿæˆæ–‡æœ¬..."):
            text_response = fetch_text_response(message_content, selected_text_model)
            if text_response:
                st.success("ç”Ÿæˆçš„æ–‡æœ¬ï¼š")
                st.write(text_response)

def excel_page():
    st.header("Excel æ–‡ä»¶è¯»å–ä¸ç¼–è¾‘")

    # Excel æ–‡ä»¶è·¯å¾„ï¼ˆå›ºå®šè·¯å¾„ï¼‰
    xlsx_path = "aisetting.xlsx"

    try:
        # è¯»å– Excel æ–‡ä»¶ä¸­çš„æ‰€æœ‰å·¥ä½œè¡¨
        df = pd.read_excel(xlsx_path, sheet_name=None)  
        sheet_names = list(df.keys())  # è·å–æ‰€æœ‰å·¥ä½œè¡¨åç§°
        selected_sheet = st.selectbox("é€‰æ‹©å·¥ä½œè¡¨", sheet_names)  # é€‰æ‹©å·¥ä½œè¡¨

        # è·å–å½“å‰å·¥ä½œè¡¨çš„æ•°æ®
        data = df[selected_sheet]
        st.write(f"**å½“å‰æ˜¾ç¤ºçš„è¡¨ï¼š{selected_sheet}**")

        # æ˜¾ç¤ºå¯ç¼–è¾‘è¡¨æ ¼
        edited_data = st.data_editor(data, use_container_width=True)  # ä½¿ç”¨ `st.data_editor`

        # æŒ‰é’®ä¿å­˜ç¼–è¾‘åçš„å†…å®¹
        if st.button("ä¿å­˜ç¼–è¾‘åçš„æ–‡ä»¶"):
            with pd.ExcelWriter(xlsx_path, engine='openpyxl') as writer:
                # éå†æ‰€æœ‰å·¥ä½œè¡¨ï¼Œä¿å­˜ç¼–è¾‘åçš„å†…å®¹
                for sheet_name, sheet_data in df.items():
                    if sheet_name == selected_sheet:
                        edited_data.to_excel(writer, index=False, sheet_name=sheet_name)  # ä¿å­˜ç¼–è¾‘çš„æ•°æ®
                    else:
                        sheet_data.to_excel(writer, index=False, sheet_name=sheet_name)  # ä¿ç•™æœªç¼–è¾‘çš„æ•°æ®

            st.success(f"å·²æˆåŠŸä¿å­˜ç¼–è¾‘åçš„å†…å®¹åˆ° {xlsx_path}")

    except Exception as e:
        st.error(f"è¯»å–æˆ–ä¿å­˜ Excel æ–‡ä»¶æ—¶å‡ºé”™: {e}")




def main():
    st.sidebar.title("å¯¼èˆª")
    page = st.sidebar.selectbox("é€‰æ‹©é¡µé¢", ["å…³é”®è¯æå–", "è¯äº‘ç”Ÿæˆ", "å›¾åƒç”Ÿæˆ", "æ–‡æœ¬ç”Ÿæˆ", "å­¦ä¹ æ—¥è¯­", "ä¸»é¢˜åˆ†æç”Ÿæˆ", "Excel"])

    if page == "å…³é”®è¯æå–":
        keyword_extraction_page()
    elif page == "è¯äº‘ç”Ÿæˆ":
        wordcloud_generation_page()
    elif page == "å›¾åƒç”Ÿæˆ":
        image_generation_page()
    elif page == "æ–‡æœ¬ç”Ÿæˆ":
        text_generation_page()
    elif page == "å­¦ä¹ æ—¥è¯­":
        japanese_learning_page()
    elif page == "ä¸»é¢˜åˆ†æç”Ÿæˆ":
        analysis_generation_page()
    elif page == "Excel":
        excel_page()  # è°ƒç”¨æ–°çš„Excelé¡µé¢å‡½æ•°

# Run the app
if __name__ == "__main__":
    main()
